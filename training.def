Bootstrap: docker
From: nvidia/cuda:12.8.0-devel-ubuntu24.04   # CUDA 12.8 + nvcc

%labels
    Author        Daniel Zhu
    Description   CUDA 12.8 • MV-MAE env • offline wheelhouse install • runs trainer_pipeline.py

# Copy the ENTIRE repo (root) into the image at /opt/app
# This expects the following at repo root: requirements.txt, wheelhouse/, trainer_pipeline.py, etc.
%files
    . /opt/app

%post
    set -e

    # --- base OS deps -------------------------------------------------------
    apt-get update -y
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        git build-essential pkg-config curl ca-certificates \
        xvfb x11-utils ffmpeg libx11-dev libxrandr-dev libxinerama-dev \
        libxcursor-dev libxi-dev mesa-common-dev gpg lsb-release libglfw3 libegl-dev \
        python3 python3-venv python3-pip
    rm -rf /var/lib/apt/lists/*

    # --- Python venv --------------------------------------------------------
    python3 -m venv /opt/mvmae_venv
    . /opt/mvmae_venv/bin/activate
    python -m pip install -U pip wheel setuptools

    # --- Robust pip settings (avoid /tmp, quiet version check) --------------
    mkdir -p /opt/pip-tmp /opt/pip-cache
    export TMPDIR=/opt/pip-tmp
    export PIP_CACHE_DIR=/opt/pip-cache
    export PIP_DISABLE_PIP_VERSION_CHECK=1
    export PIP_DEFAULT_TIMEOUT=300

    # --- OFFLINE install from local wheelhouse at /opt/app ------------------
    REQ=/opt/app/requirements.txt
    WHL=/opt/app/wheelhouse

    if [ ! -f "$REQ" ]; then
        echo "[ERROR] requirements.txt not found at $REQ"; exit 2
    fi

    if [ -d "$WHL" ]; then
        echo "[post] Installing dependencies from local wheelhouse (offline)..."
        python -m pip install --no-index --find-links "$WHL" -r "$REQ"
    else
        echo "[ERROR] wheelhouse directory missing at $WHL"
        echo "        Build is offline-only. Please create wheelhouse before building."
        exit 3
    fi

    # --- auto-activate venv & set PYTHONPATH on run -------------------------
    mkdir -p /.singularity.d/env
    cat > /.singularity.d/env/99-autovenv.sh <<'EOS'
export VIRTUAL_ENV=/opt/mvmae_venv
export PATH="$VIRTUAL_ENV/bin:$PATH"
# Make your repo (now at /opt/app) importable
export PYTHONPATH="/opt/app:${PYTHONPATH}"
EOS
    chmod 644 /.singularity.d/env/99-autovenv.sh

%environment
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    export DISPLAY=:1
    export HF_TOKEN=${HF_TOKEN:-}
    export WANDB_API_KEY=${WANDB_API_KEY:-}
    # Common headless-EGL env (MuJoCo/PyOpenGL)
    export MUJOCO_GL=egl
    export PYOPENGL_PLATFORM=egl

%runscript
    # Optional headless X for libs that insist on a DISPLAY
    if command -v xdpyinfo >/dev/null 2>&1; then
        if ! xdpyinfo -display "$DISPLAY" >/dev/null 2>&1; then
            Xvfb :1 -screen 0 1280x720x24 -nolisten tcp &  sleep 2
        fi
    fi

    echo "[MVMAE] Python: $(python --version)"
    echo "[MVMAE] PYTHONPATH=$PYTHONPATH"
    echo "[MVMAE] Running /opt/app/trainer_pipeline.py ..."

    SCRIPT="/opt/app/trainer_pipeline.py"
    if [ -f "$SCRIPT" ]; then
        cd /opt/app
        exec python -u "$SCRIPT" "$@"
    else
        echo "[ERROR] $SCRIPT not found in image."
        echo "        Falling back to passed command..."
        exec "$@"
    fi

%help
    OFFLINE build. Put "wheelhouse/" and "requirements.txt" at repo root before building.

    Build from repo root:
      apptainer build --force training.sif training.def

    Run (defaults to trainer_pipeline.py):
      apptainer run --nv training.sif --batch_size 16 --buffer_size 2000

    Exec arbitrary commands:
      apptainer exec --nv training.sif python -c "import sys, os; print(sys.path)"
