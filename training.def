Bootstrap: docker
From: nvidia/cuda:12.8.0-devel-ubuntu24.04

%labels
    Author        Daniel Zhu
    Description   MV-MAE + DrQv2 + MJX + Madrona Training Environment (CUDA 12.8, Offline bulk deps + online JAX GPU)

%files
    . /opt/src/MV_MAE_Implementation

%post
    set -e

    echo "=== [1] Installing base OS packages ==="
    apt-get update -y
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        git build-essential cmake pkg-config curl ca-certificates \
        ffmpeg libavcodec60 libavdevice60 libavfilter9 libavformat60 \
        libswscale7 libswresample4 \
        xvfb x11-utils libx11-dev libxrandr-dev libxinerama-dev \
        libxcursor-dev libxi-dev mesa-common-dev \
        libegl1 libegl-dev gpg lsb-release \
        python3 python3-venv python3-pip python3-dev
    rm -rf /var/lib/apt/lists/*

    echo "=== [2] Creating Python venv ==="
    python3 -m venv /opt/mvmae_venv
    . /opt/mvmae_venv/bin/activate
    python -m pip install -U pip setuptools wheel uv

    echo "=== [3] Configure offline wheelhouse for bulk deps ==="
    export TMPDIR=/opt/pip-tmp
    export PIP_CACHE_DIR=/opt/pip-cache
    export PIP_DISABLE_PIP_VERSION_CHECK=1
    export PIP_NO_INDEX=1
    mkdir -p /opt/pip-tmp /opt/pip-cache

    REQ=/opt/src/MV_MAE_Implementation/requirements.txt
    WHL=/opt/src/MV_MAE_Implementation/wheelhouse

    if [ ! -d "$WHL" ]; then
        echo "[ERROR] Missing wheelhouse at $WHL"
        exit 3
    fi
    if [ ! -f "$REQ" ]; then
        echo "[ERROR] Missing requirements.txt at $REQ"
        exit 2
    fi

    echo "=== [4] Installing Python deps (offline from wheelhouse) ==="
    python -m pip install --no-index --find-links "$WHL" -r "$REQ"

    echo "=== [5] Install CUDA 12.8 pinned NVIDIA libs ==="
    cat >/tmp/nvidia_cuda12_12.8.txt <<'REQ'
nvidia-cublas-cu12==12.8.3.14
nvidia-cuda-cupti-cu12==12.8.57
nvidia-cuda-nvcc-cu12==12.8.93
nvidia-cuda-nvrtc-cu12==12.8.61
nvidia-cuda-runtime-cu12==12.8.57
nvidia-cudnn-cu12==9.7.1.26
nvidia-cufft-cu12==11.3.3.41
nvidia-cufile-cu12==1.13.0.11
nvidia-curand-cu12==10.3.9.55
nvidia-cusolver-cu12==11.7.2.55
nvidia-cusparse-cu12==12.5.7.53
nvidia-cusparselt-cu12==0.6.3
nvidia-nccl-cu12==2.26.2
nvidia-nvjitlink-cu12==12.8.61
nvidia-nvtx-cu12==12.8.55
REQ

    # Let uv talk to the network for this step (donâ€™t enforce PIP_NO_INDEX here)
    unset PIP_NO_INDEX

    # Install/override to these exact CUDA12.8 wheels
    uv pip install -r /tmp/nvidia_cuda12_12.8.txt

    echo "=== [6] Installing JAX 0.4.36 with CUDA12 local plugin (GPU JAX) ==="
    # This pulls jax, jaxlib, and jax-cuda12-plugin, and wires them to the pinned CUDA libs.
    uv pip install --upgrade "jax[cuda12_local]==0.4.36"

    echo "=== [7] Build Madrona-MJX (geom_quat) ==="
    rm -rf /opt/madrona_mjx
    cd /opt
    git clone --branch geom_quat https://github.com/shacklettbp/madrona_mjx.git madrona_mjx
    cd /opt/madrona_mjx
    echo "[post] Running: git submodule update --init --recursive"
    git submodule update --init --recursive

    mkdir -p build
    cd build
    cmake .. -DLOAD_VULKAN=OFF
    make -j"$(nproc)"
    cd ..

    # Editable install so Python can import madrona_mjx
    uv pip install -e .

    # ---- NEW: make the compiled extension modules importable + ensure runtime finds Madrona libs ----
    SITE_DIR="$(python -c 'import site; print(site.getsitepackages()[0])')"
    echo "[post] site-packages: ${SITE_DIR}"
    cp -v /opt/madrona_mjx/build/_madrona_mjx_batch_renderer*.so "${SITE_DIR}/"
    cp -v /opt/madrona_mjx/build/_madrona_mjx_visualizer*.so "${SITE_DIR}/"

    echo "/opt/madrona_mjx/build" > /etc/ld.so.conf.d/madrona_mjx.conf
    ldconfig

    echo "=== [8] Auto-enable venv inside container ==="
    mkdir -p /.singularity.d/env
    cat > /.singularity.d/env/99-autovenv.sh <<'EOS'
export VIRTUAL_ENV=/opt/mvmae_venv
export PATH="$VIRTUAL_ENV/bin:$PATH"
export PYTHONPATH="/workspace:/workspace/MV_MAE_Implementation:/opt/src:${PYTHONPATH}"
EOS
    chmod 644 /.singularity.d/env/99-autovenv.sh

    echo "=== Build Completed Successfully ==="

%environment
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    export DISPLAY=:1
    export HF_TOKEN=${HF_TOKEN:-}
    export WANDB_API_KEY=${WANDB_API_KEY:-}
    export PYTHONPATH="/workspace:/workspace/MV_MAE_Implementation:/opt/src:${PYTHONPATH}"

%runscript
    if command -v nvidia-smi >/dev/null 2>&1; then
        echo "GPU:"
        nvidia-smi
    fi

    . /opt/mvmae_venv/bin/activate

    if [ -d "/workspace" ]; then
        cd /workspace
    else
        cd /opt/src
    fi

    echo "[MVMAE] Running training script..."
    exec python -m MV_MAE_Implementation.execute "$@"

%help
    Offline MV-MAE / DrQv2 / MJX + Madrona container.
    - Bulk deps (torch, mujoco, etc.) from local wheelhouse.
    - NVIDIA CUDA 12.8 wheels pinned.
    - JAX 0.4.36 with cuda12_local plugin installed from PyPI (GPU backend).
    Dev workflow:
      Bind your host repo root (the directory containing MV_MAE_Implementation/) to /workspace
      and you can add/edit files without rebuilding the SIF.

    Build:
      apptainer build training.sif training.def

    Run default training:
      apptainer run --nv --bind "$PWD:/workspace" training.sif

    Exec custom:
      apptainer exec --nv --bind "$PWD:/workspace" training.sif python3